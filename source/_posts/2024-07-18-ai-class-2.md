---
title: “第二课堂”AI实践 随堂笔记（二）：Python环境配置及基础学习
date: 2024-07-18 21:00:35
tags: [“第二课堂”AI实践]
---

# 零、课程与笔记相关信息

- [本节课的课程回放（B站）](https://www.bilibili.com/video/BV1WKbXeDEth/)
- 本笔记基于上述课程回放进行记录与补充完善，可能与实际直播略有出入。
- 本笔记部分内容摘自视频并手动码字，可能产生些许错别字及谐音字。

基于上述原因，请各位读者批评指正，向作者提出宝贵意见与建议，笔者感激不尽。

# 一、课程目标

学习 Python 语言中 `pandas` 库的基本使用方法，并使用该库处理一些简单的数据集，达成所需的目的。

# 二、课堂内容

## 1. 课程前言

这门课程得主要目的是通过真实的数据，以实战的方式了解数据分析的流程和熟悉数据分析 Python 的基本操作。知道了课程的目的之后，我们接下来我们要正式的开始数据分析的实战教学，完成 Kaggle 上泰坦尼克的任务，实战数据分析全流程。

## 2. 常用 Python 库的引入方式

Python 中的库（模块）通过 `import` 语句进行引入。特别地，对于 NumPy 与 Pandas 二库，习惯分别指定别名 `np` 与 `pd`。

```python
import numpy as np
import pandas as pd
```

## 3. 使用 Pandas 从文件中加载数据

### 普通读取

`pandas` 库提供了 `read_csv` 方法以从给定的 CSV 文件中加载数据。其中文件名既可以相对路径，又可以绝对路径的形式给出。

```python
# 以相对路径的方式指定文件名并载入数据
df = pd.read_csv('train.csv')
```

```python
# 以绝对路径的方式指定文件名并载入数据
df = pd.read_csv('/home/srcres/Coding/Projects/pandas-learn/pandas_rumen/train.csv')
```

### 逐块读取

通过指定 `read_csv` 方法的 `chunksize` 参数，可实现一次读取一定条目的逐块读取方式加载数据。例如，实现一次读取 1000 条数据的逐块加载：

```python
reader = pd.read_csv('train.csv', chunksize=1000)

for i, chunk in enumerate(reader):
    print(f"----- Chunk {i} -----")
    print(chunk)
```

## 4. 更改数据集表头名称（更改列名）

### 需求

现将样例数据集中的各英文列名翻译为中文，需要使用 Pandas 提供的列名重命名功能。翻译如下表所示：

英文 | 中文
:-: | :-:
PassengerId | 乘客ID  
Survived    | 是否幸存   
Pclass      | 乘客等级(1/2/3等舱位)  
Name        | 乘客姓名  
Sex         | 性别                 
Age         | 年龄                 
SibSp       | 堂兄弟/妹个数  
Parch       | 父母与小孩个数  
Ticket      | 船票信息             
Fare        | 票价                
Cabin       | 客舱                
Embarked    | 登船港口 

### 方式一

方式一（课程给出的方式），通过指定 `read_csv` 函数的 `names` 参数，实现在加载数据集的同时就完成列名重命名操作。

```python
df = pd.read_csv('train.csv', names=['乘客ID', '是否幸存', '乘客等级(1/2/3等舱位)', '乘客姓名', '性别', '年龄', '堂兄弟/妹个数', '父母与小孩个数', '船票信息', '票价', '客舱', '登船港口'])
```

`read_csv` 有丰富的功能各异的参数，使用得当能在加载数据时完成许多预处理操作。例如，在此基础之上再指定索引 `index` 列为 `'乘客ID'`：

```python
df = pd.read_csv('train.csv', names=['乘客ID', '是否幸存', '乘客等级(1/2/3等舱位)', '乘客姓名', '性别', '年龄', '堂兄弟/妹个数', '父母与小孩个数', '船票信息', '票价', '客舱', '登船港口'], index_col='乘客ID', header=0)
```

结果如下：

{% asset_img Screenshot_20240719_164520.png %}

### 方式二

方式二（笔者给出的方式），使用 `read_csv` 正常再入数据集，之后调用 Pandas 提供的其他功能函数进行数据集的后续处理。

```python
df = pd.read_csv('train.csv')
```

载入后，先使用 `rename` 函数完成列名更名：

```python
df = df.rename(columns={
    'PassengerId': '乘客ID',
    'Survived': '是否幸存',
    'Pclass': '乘客等级(1/2/3等舱位)',
    'Name': '乘客姓名',
    'Sex': '性别',
    'Age': '年龄',
    'SibSp': '堂兄弟/妹个数',
    'Parch': '父母与小孩个数',
    'Ticket': '船票信息',
    'Fare': '票价',
    'Cabin': '客舱',
    'Embarked': '登船港口'
})
```

接着再使用 `set_index` 函数设置数据集的索引列：

```python
df = df.set_index('乘客ID')
```

最终得到与方式一相同的结果：

{% asset_img Screenshot_20240719_164654.png %}

## 5. 数据集的初步观察

### 显示数据集的摘要

使用 `info` 函数获取数据集每一列数据的摘要。

```python
df.info()
```

{% asset_img Screenshot_20240719_170109.png %}

### 显示数据集的首末行信息

使用 `head` 或 `tail` 以获取数据集的前任意行或末尾任意行的数据信息。

```python
# 显示 df 的前 10 行信息
df.head(10)
```

{% asset_img Screenshot_20240719_170502.png %}

```python
# 显示 df 的后 15 行信息
df.tail(15)
```

{% asset_img Screenshot_20240719_170520.png %}

### 数据集空值的判断

使用 `isnull` 函数判断数据集各部分是否存在空值，并一一映射为全是 `bool` 类型数据的 `DataFrame`。

例如，显示数据集 `df` 中前5行数据的空值分布情况。

```python
df.isnull().head(5)
```

{% asset_img Screenshot_20240719_171001.png %}

## 6. 数据集的保存

对数据集 `DataFrame` 调用 `to_csv` 函数以将数据集保存为指定名称的 CSV 文件。

注意文本文件的文本编码问题。由于不同操作系统的默认文本编码不同（Windows 上默认为 `GBK`，GNU/Linux 上默认为 `UTF-8`），如所保存的文件出现乱码现象，就需要通过 `to_csv` 函数的 `encoding` 参数手动指定文件编码。

例如，将 `df` 保存到 `train_chinese.csv` 文件中，并指定文本编码为 `utf-8`：

```python
df.to_csv('train_chinese.csv', encoding='utf-8')
```

## 7. 获取数据集的所有列

通过数据集 `DataFrame` 的 `columns` 属性，可以轻松地获取其所有列的名称。例如：

```python
print('\n'.join(list(df.columns)))
```

{% asset_img Screenshot_20240719_172228.png %}

## 8. 数据集的列操作

### 理论储备

要获取数据集某一列的所有值，可使用下标索引语法，给定需要进行获取的列名即可。得到的结果是 `Series` 类型的。

例如，查询 `df` 中 `'Cabin'` 一列中的所有值：

```python
df['Cabin']
```

{% asset_img Screenshot_20240719_172739.png %}

或者，也可通过点语法获取。Pandas 已将各列数据包装成为 `DataFrame` 的属性，只需访问列名对应的属性即可：

```python
df.Cabin
```

{% asset_img Screenshot_20240719_173410.png %}

> **小记**
>
> 亦可使用 `DataFrame` 的 `loc` 属性，通过筛选出对应列的数据，实现列数据的获取。
>
> ```python
> df.loc[:, 'Cabin']
> ```
>
> {% asset_img Screenshot_20240719_174639.png %}

### 实战演练

#### 需求

加载文件 `test_1.csv`，然后对比 `train.csv`，看看有哪些多出的列，然后将多出的列删除。

#### 实现

先将上述两个数据集进行加载：

```python
# train.csv 即为前文数据集对象 df，省略其加载步骤。
df_test_1 = pd.read_csv('test_1.csv')
```

接下来通过简单的算法，筛选出题目所要求的多余的列：

```python
df_cols = list(df.columns)
dup_cols = [] # 多余的列

for col in list(df_test_1.columns):
    if col not in df_cols:
        dup_cols.append(col)
```

最后使用 `drop` 函数将 `df_test_1` 中的多余列删除。务必注意指定 `axis` 参数：

```python
df_test_1 = df_test_1.drop(dup_cols, axis=1)
```

结果如下：

{% asset_img Screenshot_20240719_174229.png %}

> **小记**
>
> 亦可通过 Python 的 `del` 关键字对 `DataFrame` 的列进行删除。如上述代码：
>
> ```python
> df_test_1 = df_test_1.drop(dup_cols, axis=1)
> ```
>
> 可修改为等价代码：
>
> ```python
> for col in dup_cols:
>     del df_test_1[col]
> ```

## 9. 数据集的隐藏

`DataFrame` 中有个 `style` 属性，该属性控制 `DataFrame` 在笔记本或终端中被展示出来时的样式。通过调用 `style` 属性的 `hide` 方法，能将 `DataFrame` 中特定的部分隐藏起来，从而不会在笔记本或终端中被展示出来。

例如，将 `df_test_1` 中 `'PassengerId'`、`'Name'`、`'Age'`、`'Ticket'` 这些列隐藏起来：

```python
df_test_1.style.hide(['PassengerId', 'Name', 'Age', 'Ticket'], axis=1)
```

结果如下：（~~此处我的 IDE 抽风，不显示结果，应该是 IDE 的 bug = =，被迫切换网页版的 Jupyter Notebook。~~）

{% asset_img Screenshot_20240719_180119.png %}

> **注意**
>
> 此处知识点，课程所介绍的隐藏数据方式有误。`drop` 函数与 `del` 关键字相同，均是删除 `DataFrame` 中的数据，而**非**隐藏数据。要隐藏数据不被展示出来，而不破坏原有的数据结构，只能通过操作 `style` 属性来实现，如上所述。
>
> {% asset_img Screenshot_20240719_180616.png %}

## 10. 数据集的筛选

Pandas 支持多种方式从数据集中筛选符合给定指定的数据。

### 题目一

题目一：从数据集 `df` 中筛选出 `'Age'` 在10岁以下的乘客信息。

在 Pandas 中，我们可以使用 `DataFrame` 的 `loc` 属性来实现，并传入我们的查询条件：

```python
df.loc[df['Age'] < 10, :]
```

观察所得数据，其 `'Age'` 值均小于10，符合我们的预期。

{% asset_img Screenshot_20240719_193452.png %}

> **小记**
>
> 亦可不使用 `loc` 属性，直接使用查询条件作为索引，效果等价。
>
> ```python
> df[df['Age'] < 10]
> ```
>
> {% asset_img Screenshot_20240719_193829.png %}

### 题目二

题目二：以 `'Age'` 为条件，将年龄在10岁以上和50岁以下的乘客信息显示出来，并将这个数据命名为 `midage`。

在 Pandas 中，筛选条件之间的二元运算使用 `&` 和 `|` 运算符，其中 `&` 表示“并且”，`|` 表示“或者”。

例如，要实现题设所给的筛选条件，先分别写出表示“10岁以上”和“50岁以下”的条件，再将其用 `&` 连接。注意连接前每个参与运算的条件需用括号括起来，否则会因为默认的运算符优先级而导致代码逻辑背离我们的期望。

```python
# 注意将查询结果存入变量 midage 中
midage = df.loc[(df['Age'] >= 10) & (df['Age'] < 50), :]
```

所得结果，数据的 `'Age'` 值的分布范围符合我们的预期。

{% asset_img Screenshot_20240719_195622.png %}

> **小记**
> 同样地，若不使用 `loc`，也能如下达到相同的效果：
>
> ```python
> midage = df[(df['Age'] >= 10) & (df['Age'] < 50)]
> ```

### 题目三

题目三：将 `midage` 的数据中第100行的 `'Pclass'` 和 `'Sex'` 的数据显示出来。

此处就必须用到前文中常提到的 `loc` 属性了。对 `loc` 属性同时在行与列上进行索引，以获取到题目所要求的各项数据。其中索引需要提供两个参数，第一个为行，第二个为列。若要查询多行或多列，将对应位置改为列表即可。

**尤其注意**的是，本题要求获取到 `midage` 中第100行的数据。因此在查询操作前，务必确保该数据集的索引是默认索引，先调用 `reset_index` 将其重置为默认索引。否则可能会因为索引未重置导致查询到其他行的数据，或者无法查询到数据。

```python
# 查询前先重置索引
midage = midage.reset_index()
# 查询第100行数据，因索引值从0开始计算，故指定索引为99
midage.loc[99, ['Pclass', 'Sex']]
```

所得结果是 `Series` 类型的。这是显然的；因为我们仅查询了一行数据。

{% asset_img Screenshot_20240719_200621.png %}

### 题目四

题目四：使用 `loc` 方法将 `midage` 的数据中第100，105，108行的 `Pclass`，`Name` 和 `Sex` 的数据显示出来。

与上一题类似，但此时对 `loc` 的索引要同时指定多行和多列。

```python
midage.loc[[99, 104, 107], ['Pclass', 'Name', 'Sex']]
```

此时所得到的结果就是 `DataFrame` 类型的了，因为我们同时查询了多行与多列的数据。

{% asset_img Screenshot_20240719_202743.png %}

### 题目五

题目五：使用 `iloc` 方法将 `midage` 的数据中第100，105，108行的 `Pclass`，`Name` 和 `Sex` 的数据显示出来。

`iloc` 与 `loc` 非常类似，只是其中对列的索引由列名的方式改为了整数下标的方式。

```python
midage.iloc[[99, 104, 107], [3, 4, 5]]
```

结果与上一题一致。

{% asset_img Screenshot_20240719_203205.png %}

## 11. 数据集的分析

在前面我们已经学习了 Pandas 基础，知道利用 Pandas 读取 csv 数据的增删查改，今天我们要学习的就是**探索性数据分析**，主要介绍如何利用 Pandas 进行排序、算术计算以及计算描述函数 `describe()` 的使用。

### 对数据进行排序

#### 理论储备

给定下列样例数据集：

```python
frame = pd.DataFrame(np.arange(8).reshape((2, 4)), 
                     index=['2', '1'], 
                     columns=['d', 'a', 'b', 'c'])
```

{% asset_img Screenshot_20240719_204153.png %}

**操作一：**将该数据集按任意列升序排列。

在 Pandas 中，排列数据集的方式是调用该数据集的 `sort_values` 方法。该方法需要指定两个参数，`by` 参数用以指明排序时所参考的行或列， `ascending` 标明是否升序排序，`True` 为升序排序，`False` 为降序排序；默认值为 `True`，即升序排序。

选取列 `'d'` 作为示例，按该列进行升序排序。

```python
frame.sort_values('d', ascending=True)
```

因 `by` 为 `sort_values` 方法的第一个参数，故可省略其参数名。

{% asset_img Screenshot_20240722_212128.png %}

**操作二：**将该数据集让行索引升序排序。

按行排序的操作与按列排序时类似，将 `axis` 参数指定为 `1`，`by` 参数由参照列改为参照行即可。注意此处的行指该数据集的 `index` 属性中的元素。

例如，按行 `'2'` 升序排序。

```python
frame.sort_values('2', ascending=True, axis=1)
```

{% asset_img Screenshot_20240722_212703.png %}

**操作三：**将该数据集让行索引降序排序。

参照上面的例子稍作修改即可，将 `ascending` 参数修改为 `False`。

```python
frame.sort_values('2', ascending=False, axis=1)
```

{% asset_img Screenshot_20240722_212911.png %}

**操作四：**将该数据集让任选两列数据同时降序排序。

以 `'d'` 列为例。

```python
frame.sort_values('d', ascending=False, axis=0)
```

{% asset_img Screenshot_20240722_213232.png %}

**操作五：**将该数据集让任选两列数据同时降序排序。

`sort_values` 方法也是支持同时指定多行或多列作为排序时的参考的，只需将 `by` 参数中的列修改为由多个列组成的 `list` 即可。

例如，同时指定 `'a'` `'b'` 两列作为排序参考，进行降序排序。

```python
frame.sort_values(['a', 'b'], ascending=False, axis=0)
```

{% asset_img Screenshot_20240722_213518.png %}

#### 实战演练

##### 需求

对泰坦尼克号数据（`train.csv`）按票价和年龄两列进行综合排序（降序排列），从数据中你能发现什么。

##### 分析

分析需求知，我们需要对 `train.csv` 数据集进行：

1. 对列降序排序；
2. 参考列为 `'票价'` 和 `'年龄'`。

再套用前文所述前置知识即可。

##### 实现

```python
# 此处使用了 head(20) 方法精简输出，便于观察结果
df.sort_values(['票价', '年龄'], ascending=False, axis=0).head(20)
```

{% asset_img Screenshot_20240722_214137.png %}

观察结果，数据集中年龄和票价随着数据行号的增加而递减，符合预期。

{% asset_img Screenshot_20240722_214322.png %}

### 数据集的二元运算

Pandas 中的 `DataFrame` 是支持数据集之间的二元运算的。下面以 `+` 运算为例。

给定如下两份样例数据：

```python
frame1_a = pd.DataFrame(np.arange(9.).reshape(3, 3),
                     columns=['a', 'b', 'c'],
                     index=['one', 'two', 'three'])
frame1_b = pd.DataFrame(np.arange(12.).reshape(4, 3),
                     columns=['a', 'e', 'c'],
                     index=['first', 'one', 'two', 'second'])
```

{% asset_img Screenshot_20240722_214652.png %}

现将 `frame1_a` 与 `frame1_b` 相加：

```python
frame1_a + frame1_b
```

{% asset_img Screenshot_20240722_214900.png %}

可以看到，两个 `DataFrame` 相加后，会返回一个新的 `DataFrame` ，对应的行和列的值会相加，没有对应的会变成空值 `NaN`。

当然，`DataFrame` 还有很多算术运算，如减法，除法。此处从略。

### 对数据集进行统计

接下来对上述 `train.csv` 数据集进一步分析。我们希望通过泰坦尼克号数据，计算出在船上最大的家族有多少人。其中对于家族人数的定义为：家族人数=兄弟姐妹个数+父母子女个数。

结合前面已学的知识，可以运用 `DataFrame` 的 `apply` 方法简单地达成目的。

```python
count_sum = df.apply(lambda it: it['堂兄弟/妹个数'] + it['父母与小孩个数'], axis=1)
print(count_sum.max())
```

{% asset_img Screenshot_20240722_215544.png %}

### 查看数据集的基本统计信息

#### 理论储备

Pandas 的 `DataFrame` 提供了一个 `describe` 函数，方便我们查看数据基本统计信息。

例如，对于以下样例数据集：

```python
frame2 = pd.DataFrame([[1.4, np.nan], 
                       [7.1, -4.5],
                       [np.nan, np.nan], 
                       [0.75, -1.3]
                      ], index=['a', 'b', 'c', 'd'], columns=['one', 'two'])
```

{% asset_img Screenshot_20240722_220106.png %}

使用 `describe` 方法查看其基本统计信息：

```
frame2.describe()
```

{% asset_img Screenshot_20240722_220222.png %}

#### 实战演练

##### 需求

查看 `train.csv` 数据集中 票价、父母子女 这列数据的基本统计数据，你能发现什么。

##### 分析

根据需求，使用 `describe` 方法分别查看 `df` 数据集中 `'票价'` 和 `'父母与小孩个数'` 两列的基本统计信息，并人工分析得出自己对于这些数据的认知。

##### 实现

首先查看 `'票价'` 列的基本统计信息。

```python
df['票价'].describe()
```

{% asset_img Screenshot_20240722_220636.png %}

观察该统计数据可知，此列共有 `891` 个数据，其平均值为 `32.204208`，标准差为 `49.693429`，最小值为 `0.000000`；第 `25%` 分位数为 `7.910400`，第 `50%` 分位数为 `14.454200`，第 `75%` 分位数为 `31.000000`，最大值为 `512.329200`。

再来查看 `'父母与小孩个数'` 列的基本统计信息。

```python
df['父母与小孩个数'].describe()
```

{% asset_img Screenshot_20240722_221042.png %}

观察该统计数据可知，此列共有 `891` 个数据，其平均值为 `30.381594`，标准差为 `0.806057`，最小值为 `0.000000`；第 `25%` 分位数为 `0.000000`，第 `50%` 分位数为 `0.000000`，第 `75%` 分位数为 `0.000000`，最大值为 `0.000000`。

# 三、课后作业

## 作业一

{% asset_img Screenshot_20240722_223106.png %}

本作业的相关内容已融合进前文的笔记部分中，此处恕不赘述。可参看笔者提交的作业文件。

## 作业二

### 题目

{% asset_img Screenshot_20240722_223329.png %}

### 分析

本作业要求我们按照题目的要求，使用 Anaconda 创建给定条件下的虚拟环境，并使用该虚拟环境对给定的数据集 `pokemon.csv` 进行处理。在电脑上已安装完毕 Anaconda 的前提条件下，只需按照要求一步步满足即可。

### 实现

#### 环境准备

先[前往下载](https://eumgrqwyfu.feishu.cn/docx/LfNpdTGmXo79EuxPtyPckOJ9nYd)所需的数据集 `pokemon.csv`，以便完成后续事宜。

鉴于题目要求我们使用给定名称的全新 Anaconda 虚拟环境（姓名首字母小写），打开终端并确保已激活到 Anaconda 上下文中。若尚未激活，先运行：

```shell
conda activate base
```

进入到 Anaconda 的基础环境中。然后新建虚拟环境（笔者起名为 `hhw`）：

```shell
conda create -n hhw
```

等待一会儿，让虚拟环境创建完成；中途可能弹出确认选项 `[y/N]`，输入 `y` 后回车即可。

创建完毕后，进入到这个新的虚拟环境中：

```shell
conda activate hhw
```

并确保终端前方的小括号内的虚拟环境名为刚才所起的名字（`hhw`），以防后续操作出错。

{% asset_img Screenshot_20240723_194806.png %}

为在这个新环境中使用 Jupyter Notebook，还需提前配置这个虚拟环境，安装必要的 Anaconda 软件包：

```shell
conda install pip ipykernel jupyter
```

等待安装完成；同样可能出现确认选项 `[y/N]`，按前文所述方式操作即可。

接下来还需使用 `pip` 安装 `pandas` 软件包，因为 Anaconda 默认是不带这个软件包的。

```shell
pip install pandas
```

等待安装完成。至此我们的新虚拟环境配置完成，可以启动 Jupyter Notebook 继续完成作业了。

```shell
jupyter notebook
```

来处理题目所给的 `pokemon.csv` 数据集。我们使用 Python 处理该数据集，首先得导入相关模块，切记不能忘。这里因为笔者希望将程序的输出数据存入子文件夹 `run` 中，所以在导入相关模块后也进行了该文件夹的检测与创建。

```python
import os
import pandas as pd

if not os.path.exists('run'):
    os.mkdir('run')
```

使用 Pandas 处理数据集之前先要将其加载到 Python 中，并使用 `head` 方法适当查看验证数据集是否准确无误。

```python
df = pd.read_csv('pokemon.csv')
df.head()
```

{% asset_img Screenshot_20240723_200239.png %}

载入数据集后，来完成题目给出的一系列要求。

#### 要求一

要求一：将列名更换为中文。借助 `DataFrame` 的 `rename` 方法来实现：

```python
df.rename(columns={
    'Name': '名称',
    'Type 1': '类型1',
    'Type 2': '类型2',
    'Total': '总计',
    'HP': '生命值',
    'Attack': '攻击力',
    'Defense': '防御力',
    'Sp. Atk': '特别攻击力',
    'Sp. Def': '特别防御力',
    'Speed': '速度'
})
```

{% asset_img Screenshot_20240723_200414.png %}

> **注意**
>
> 由于笔者所使用的操作系统存在输入法问题，不能在代码编辑器中输入中文，故后续代码中将保留使用纯英文。

#### 要求二

要求二：分别存储单属性与双属性的妖怪，存储为不同 csv 文件。

先借助 `DataFrame` 的 `loc` 属性筛选出单属性妖怪。其中用到了 Pandas 提供的 `isna` 函数，该函数判断某一列中各位置是否为 `NaN`，并映射为一个 `bool` 值列。观察数据集的特征知单属性妖怪，其第二属性 `Type 2` 一列的值均为 `NaN`，故可借助该特性筛选单属性妖怪。

```python
single = df.loc[pd.isna(df['Type 2']), :]
single
```

观察结果，均为单属性，符合我们的期望。

{% asset_img Screenshot_20240723_201012.png %}

最后别忘了将筛选出来的数据储存为 csv 文件。

```python
single.to_csv('run/single.csv', header=True)
```

再来筛选双属性妖怪；同样借助 `loc` 属性和 `isna` 函数。与单属性相反的就是双属性，因此需要对 `isna` 函数的返回值进行取反操作；这点可以借助 `Series` 的 `map` 方法来实现。

```python
double = df.loc[pd.isna(df['Type 2']).map(lambda it: not it), :]
double
```

观察结果，均为双属性，符合我们的期望。

{% asset_img Screenshot_20240723_201627.png %}

同样地别忘了保存为 csv 文件。

```python
double.to_csv('run/double.csv', header=True)
```

#### 要求三

要求三：对于 `#` 重复的妖怪只保留第一条记录，并求出第一属性的种类数量和前三数量对应的种类。

`Index` 类型提供了一个 `duplicated` 方法，用以查找出数据集各行中重复的索引。不过因为我们希望保留不重复的索引，所以同样要对该方法的返回值取反。使用该方法与 `loc` 属性结合即可达到这个要求。

```python
df2 = df.reset_index()
df2 = df2.loc[pd.Series(df.index.duplicated(keep='first')).map(lambda it: not it), :]
df2 = df2.set_index('#')
df2
```

观察结果，`#` 重复的各行数据已被去除，符合我们的期望。

{% asset_img Screenshot_20240723_202431.png %}

最后，别忘了将结果保存为 csv 文件。

```python
double.to_csv('run/double.csv', header=True)
```

之后来求出第一属性的种类数量和前三数量对应的种类。

我们可以使用 `Series` 的 `value_counts` 方法来统计列的不同数据种类的数量。

```python
print('The number of types of Type 1 within the dataset:', df2['Type 1'].value_counts().shape[0])
```

{% asset_img Screenshot_20240723_203205.png %}

要查询在数量上名列前茅的列的数据种类，先将 `value_counts` 方法返回的 `Series` 进行按数量上的降序排序，进而实现查询数量上排前几的数据种类。

```python
print(
    'Top 3 types of Type 1 in amount:',
    ', '.join(list(
        df2['Type 1'].value_counts()
            .to_frame(name='count')
            .sort_values(by='count', ascending=False)
            .index[:3]
    ))
)
```

{% asset_img Screenshot_20240723_204001.png %}

#### 要求四

要求四：求第一属性和第二属性的组合种类。

得益于 `DataFrame` 类型提供的 `apply` 方法，我们可以方便地对数据集的各列内容进行组合。

```python
bitypes = list(
    df2.loc[df2.apply(lambda it: type(it['Type 2']) is str, axis=1), :]
        .apply(lambda it: f"{it['Type 1']},{it['Type 2']}", axis=1)
        .value_counts()
        .index
)
print('\n'.join(bitypes))
```

{% asset_img Screenshot_20240723_204540.png %}

#### 要求五

要求五：找出Total, HP, Attack, Defense, Sp. Atk, Sp. Def, Speed的最大值，平均值，方差，中位数。

依次调用 `Series` 类型提供的统计方法 `max`、`mean`、`std` 以及 `median` 即可。

```python
cols = [
    'Total', 'HP', 'Attack', 'Defense', 'Sp. Atk',
    'Sp. Def', 'Speed'
]
methods = ['max', 'mean', 'std', 'median']

for col in cols:
    print(f"[{col}]")
    for method in methods:
        print(f"{method} =", getattr(df2[col], method)())
    print()
```

{% asset_img Screenshot_20240723_205034.png %}

#### 要求六

要求六：将 `HP` 超过 `100` 的替换为 `high`，不足 `50` 的替换为 `low`，否则设为 `mid`。

同样地，借助 `DataFrame` 的 `apply` 方法轻松实现。

```python
def get_hp_level(x):
    hp = int(x['HP'])
    if hp > 100:
        return 'high'
    elif hp < 50:
        return 'low'
    else:
        return 'mid'
    
df3 = df2.copy()
df3['HP'] = df3['HP'].astype('object')
df3.loc[:, 'HP'] = df3.apply(get_hp_level, axis=1)
df3
```

{% asset_img Screenshot_20240723_205522.png %}

#### 要求七

要求七：对 `total` 值进行降序排序并存储为 csv 文件。

使用 `DataFrame` 的 `sort_values` 方法对数据集进行排序。

```python
df3 = df3.sort_values(by='Total', ascending=False)
df3
```

{% asset_img Screenshot_20240723_210510.png %}

最后别忘了将排序后的数据保存为 csv 文件。

```python
df3.to_csv('run/final.csv', header=True)
```

### 心得

本作业针对前面所学的 Pandas 各方面的知识进行了充分的演练，让学生在掌握了使用 Pandas 软件包处理数据集的相关理论知识的同时，提供了样例数据集让学生按照指引实操对数据集的处理，并存储处理完成的数据。

同时本次作业也改掉了上节课的作业中笔者所指出的作业覆盖面不足的问题，即作业不能完整地覆盖本节课所讲的全部知识点。这使得学生对知识点的掌握更加系统、更加全面，为后续基于 Pandas 的数据分析与处理的课程内容打下坚实基础。
